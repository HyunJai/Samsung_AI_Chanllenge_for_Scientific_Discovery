{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "baseline.ipynb <br>\n",
    ".. ├ models <br>\n",
    ".. └ data <br>\n",
    ".... ├ train_sdf <br>\n",
    ".... ├ dev_sdf <br>\n",
    ".... ├ test_sdf <br>\n",
    ".... ├ train_imgs <br>\n",
    ".... ├ test_imgs <br>\n",
    ".... ├ sample_train.csv <br>\n",
    ".... ├ sample_test.csv <br>\n",
    ".... └ sample_submission.csv <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사용 패키지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "6opPu4z_p_wb"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import rdkit\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit import RDLogger\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler, ConcatDataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch.nn import functional as F\n",
    "import random\n",
    "\n",
    "from albumentations import (\n",
    "    Compose, OneOf, Normalize, Resize, RandomResizedCrop, RandomCrop, HorizontalFlip, VerticalFlip, \n",
    "    RandomBrightness, RandomContrast, RandomBrightnessContrast, Rotate, ShiftScaleRotate, Cutout, \n",
    "    IAAAdditiveGaussianNoise, Transpose, Blur\n",
    "    )\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from albumentations import ImageOnlyTransform\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from torch_poly_lr_decay import PolynomialLRDecay\n",
    "from alphafold2_pytorch import Alphafold2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy verison : 1.19.2\n",
      "pandas version : 1.1.5\n",
      "opencv version : 4.5.3\n",
      "rdkit version : 2021.03.4\n",
      "torch version : 1.7.1\n"
     ]
    }
   ],
   "source": [
    "print('numpy verison :', np.__version__)\n",
    "print('pandas version :', pd.__version__)\n",
    "print('opencv version :', cv2.__version__)\n",
    "print('rdkit version :', rdkit.__version__)\n",
    "print('torch version :', torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "7WZfRhyfp_wd"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('/home/ubuntu/Samsung_Dacon/train2.csv')\n",
    "dev = pd.read_csv('/home/ubuntu/Samsung_Dacon/dev.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 하이퍼파라미터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "mBCHZd0Gp_wg"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "BATCH_SIZE = 356\n",
    "EPOCHS = 400\n",
    "num_layers = 1\n",
    "dropout_rate = 0.1\n",
    "atten_dim = 128\n",
    "embedding_dim = 256\n",
    "learning_rate = 1e-3\n",
    "vision_pretrain = True\n",
    "save_path = f'/home/ubuntu/Samsung_Dacon/models/EFB1_test.pt'\n",
    "\n",
    "# best : tf_efficientnet_b1 / AdamW / Lr : 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMILES Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "XP7FhxPLp_wf"
   },
   "outputs": [],
   "source": [
    "class SMILES_Tokenizer():\n",
    "    def __init__(self, max_length):\n",
    "        self.txt2idx = {}\n",
    "        self.idx2txt = {}\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def fit(self, SMILES_list):\n",
    "        unique_char = set()\n",
    "        for smiles in SMILES_list:\n",
    "            for char in smiles:\n",
    "                unique_char.add(char)\n",
    "        unique_char = sorted(list(unique_char))\n",
    "        for i, char in enumerate(unique_char):\n",
    "            self.txt2idx[char]=i+2\n",
    "            self.idx2txt[i+2]=char\n",
    "            \n",
    "    def txt2seq(self, texts):\n",
    "        seqs = []\n",
    "        for text in tqdm(texts):\n",
    "            seq = [0]*self.max_length\n",
    "            for i, t in enumerate(text):\n",
    "                if i == self.max_length:\n",
    "                    break\n",
    "                try:\n",
    "                    seq[i] = self.txt2idx[t]\n",
    "                except:\n",
    "                    seq[i] = 1\n",
    "            seqs.append(seq)\n",
    "        return np.array(seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Aj9oVC7Xp_wf",
    "outputId": "46e31b71-ae36-4ed9-f182-d0d2f806fceb"
   },
   "outputs": [],
   "source": [
    "max_len = train.SMILES.str.len().max()\n",
    "max_len\n",
    "tokenizer = SMILES_Tokenizer(max_len)\n",
    "tokenizer.fit(train.SMILES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v8TdltRhp_wf",
    "outputId": "62b78ec4-164d-4e35-ad03-d22ea01a83ad"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30274/30274 [00:00<00:00, 48247.94it/s]\n"
     ]
    }
   ],
   "source": [
    "seqs = tokenizer.txt2seq(train.SMILES)\n",
    "labels = train[['Gap']].to_numpy()\n",
    "imgs = ('/home/ubuntu/Samsung_Dacon/train_imgs/'+train.uid+'.png').to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "DxosU_4Kp_wg",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((27000,), (27000, 265), (27000, 1), (3274,), (3274, 265), (3274, 1))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "imgs, seqs, labels = shuffle(imgs, seqs, labels, random_state=42)\n",
    "\n",
    "train_imgs = imgs[:27000]\n",
    "train_seqs = seqs[:27000]\n",
    "train_labels = labels[:27000]\n",
    "\n",
    "val_imgs = imgs[27000:]\n",
    "val_seqs = seqs[27000:]\n",
    "val_labels = labels[27000:]\n",
    "\n",
    "train_imgs.shape, train_seqs.shape, train_labels.shape, val_imgs.shape, val_seqs.shape, val_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "\n",
    "def seed_torch(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_torch(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Ir5slCDRp_wg"
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, imgs, seqs, labels=None, mode='train', transform=None, preprocessing=None):\n",
    "        self.mode = mode\n",
    "        self.imgs = imgs\n",
    "        self.seqs = seqs\n",
    "        if self.mode=='train':\n",
    "            self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.preprocessing = preprocessing\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        img = cv2.imread(self.imgs[i]).astype(np.float32)/255\n",
    "        img = np.transpose(img, (2,0,1))\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=img)\n",
    "            img = augmented['image']\n",
    "        if self.preprocessing:\n",
    "            pr = self.preprocessing(image=img)\n",
    "            img = pr['image']\n",
    "        \n",
    "        if self.mode == 'train':\n",
    "            return {\n",
    "                'img' : torch.tensor(img, dtype=torch.float32),\n",
    "                'seq' : torch.tensor(self.seqs[i], dtype=torch.long),\n",
    "                'label' : torch.tensor(self.labels[i], dtype=torch.float32)\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'img' : torch.tensor(img, dtype=torch.float32),\n",
    "                'seq' : torch.tensor(self.seqs[i], dtype=torch.long),\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_augmentation(y=299,x=299):\n",
    "    return A.Compose([\n",
    "        A.Resize(y,x),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "#         A.VerticalFlip(p=0.5),\n",
    "        A.Rotate(90)\n",
    "        ])\n",
    "\n",
    "\n",
    "formatted_settings = {\n",
    "            'input_size': [3, 300, 300],\n",
    "            'input_range': [0, 1]\n",
    "}\n",
    "def preprocess_input(\n",
    "    x, mean=None, std=None, input_space=\"RGB\", input_range=None, **kwargs\n",
    "):\n",
    "\n",
    "    if input_space == \"BGR\":\n",
    "        x = x[..., ::-1].copy()\n",
    "\n",
    "    if input_range is not None:\n",
    "        if x.max() > 1 and input_range[1] == 1:\n",
    "            x = x / 255.0\n",
    "\n",
    "    if mean is not None:\n",
    "        mean = np.array(mean)\n",
    "        x = x - mean\n",
    "\n",
    "    if std is not None:\n",
    "        std = np.array(std)\n",
    "        x = x / std\n",
    "\n",
    "    return x\n",
    "\n",
    "def get_preprocessing(preprocessing_fn):\n",
    "    _transform = [\n",
    "#         A.Lambda(image=preprocessing_fn),\n",
    "#         A.Lambda(image=to_tensor, mask=to_tensor),\n",
    "#         ToTensorV2\n",
    "    ]\n",
    "    return A.Compose(_transform)\n",
    "\n",
    "def get_validation_augmentation(y=299,x=299):\n",
    "    return A.Compose([\n",
    "        A.Resize(y,x),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "#         A.VerticalFlip(p=0.5),\n",
    "        A.Rotate(90)\n",
    "        ])\n",
    "\n",
    "def to_tensor(x, **kwargs):\n",
    "    \"\"\"\n",
    "    Convert image or mask.\n",
    "    \"\"\"\n",
    "    return x.transpose(2, 0, 1).astype('float32')\n",
    "\n",
    "def norm(img):\n",
    "    img-=img.min()\n",
    "    return img/img.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "GJfwH3Cop_wh"
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "preprocessing_fn = functools.partial(preprocess_input, **formatted_settings)\n",
    "train_dataset = CustomDataset(train_imgs, train_seqs, train_labels,\n",
    "                              transform=get_training_augmentation(),\n",
    "                             preprocessing=get_preprocessing(preprocessing_fn)\n",
    "                             )\n",
    "val_dataset = CustomDataset(val_imgs, val_seqs, val_labels,\n",
    "                           transform=get_validation_augmentation(),\n",
    "                           preprocessing=get_preprocessing(preprocessing_fn)\n",
    "                           )\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               num_workers=12,\n",
    "                                               shuffle=True,\n",
    "                                               pin_memory=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                             batch_size=BATCH_SIZE,\n",
    "                                             num_workers=12,\n",
    "                                             shuffle=False,\n",
    "                                             pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PmG7KpwYp_wh"
   },
   "outputs": [],
   "source": [
    "sample_batch = next(iter(train_dataloader))\n",
    "sample_batch['img'].size(), sample_batch['seq'].size(), sample_batch['label'].size()\n",
    "sample_batch['img'].dtype, sample_batch['seq'].dtype, sample_batch['label'].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델\n",
    "\n",
    "- EfficientNet에 LSTM을 연결한 CNN2RNN 모델\n",
    "\n",
    "### EfficientNet n_channels_dict\n",
    "- B1 : 1280 / B2 : 1408 / B3 : 1536 / B4 : 1792 / B5 : 2048 / B6 : 2304 / B7 : 2560"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "\n",
    "class CNN_Encoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, rate):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        model = timm.create_model('tf_efficientnet_b1', pretrained=True)\n",
    "        modules = list(model.children())[:-2]\n",
    "        self.feature_extract_model = nn.Sequential(*modules)\n",
    "        self.dropout1 = nn.Dropout(rate)\n",
    "        self.fc = nn.Linear(1280, embedding_dim)\n",
    "        # ResNetRs : 2048\n",
    "        self.dropout2 = nn.Dropout(rate)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.feature_extract_model(x)\n",
    "        x = x.permute(0,2,3,1)\n",
    "        x = x.view(x.size(0), -1, x.size(3))\n",
    "        x = self.dropout1(x)\n",
    "        x = nn.ReLU()(self.fc(x))\n",
    "        x = self.dropout2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.nn.utils.rnn import pad_packed_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version 1 : Bi-LSTM * 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Decoder(nn.Module):\n",
    "    def __init__(self, max_len, embedding_dim, num_layers, rate):\n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(max_len, embedding_dim)\n",
    "        self.dropout = nn.Dropout(rate)\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "        self.lstm1 = nn.LSTM(embedding_dim, embedding_dim, num_layers,\n",
    "                             bidirectional=True, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(embedding_dim*2, embedding_dim, num_layers,\n",
    "                            bidirectional=True, batch_first=True)\n",
    "        self.final_layer1 = nn.Linear(embedding_dim*2, embedding_dim*2)\n",
    "        self.final_layer2 = nn.Linear(embedding_dim*2, embedding_dim*2)\n",
    "        self.linear_global = nn.Linear(embedding_dim*2, 1)\n",
    "\n",
    "    def forward(self, enc_out, dec_inp):\n",
    "        with torch.no_grad():\n",
    "            embedded = self.embedding(dec_inp)\n",
    "        \n",
    "        embedded = self.dropout(embedded)\n",
    "        embedded = torch.cat([enc_out, embedded], dim=1)\n",
    "        self.lstm1.flatten_parameters()\n",
    "        hidden1, _ = self.lstm1(embedded)\n",
    "        \n",
    "        self.lstm2.flatten_parameters()\n",
    "        hidden2, _ = self.lstm2(hidden1)\n",
    "        \n",
    "        h_conc_linear1 = F.relu(self.final_layer1(hidden1))\n",
    "        h_conc_linear2 = F.relu(self.final_layer2(hidden2))\n",
    "        \n",
    "        \n",
    "        hidden = hidden1 + hidden2 + h_conc_linear1 + h_conc_linear2\n",
    "        \n",
    "        \n",
    "#         output = nn.ReLU()(self.final_layer(hidden))\n",
    "        output = self.linear_global(hidden.mean(1))\n",
    "#         output = hidden.mean(1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version 2 : Bi-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Decoder(nn.Module):\n",
    "    def __init__(self, max_len, embedding_dim, num_layers, rate):\n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(max_len, embedding_dim)\n",
    "        self.dropout = nn.Dropout(rate)\n",
    "        self.lstm = nn.LSTM(embedding_dim, embedding_dim, num_layers,\n",
    "                             bidirectional=True, batch_first=True)\n",
    "        self.linear_first = nn.Linear(2*embedding_dim, embedding_dim)\n",
    "        self.linear_first.bias.data.fill_(0)\n",
    "        self.linear_second = nn.Linear(embedding_dim, 1)\n",
    "        self.linear_second.bias.data.fill_(0)\n",
    "        \n",
    "    def forward(self, enc_out, dec_inp):\n",
    "        embedded = self.embedding(dec_inp)\n",
    "        \n",
    "        embedded = self.dropout(embedded)\n",
    "        embedded = torch.cat([enc_out, embedded], dim=1)\n",
    "#         self.lstm1.flatten_parameters()\n",
    "        outputs, _ = self.lstm(embedded)\n",
    "        \n",
    "        outputs = outputs[:,-1,:]\n",
    "        outputs = torch.flatten(outputs, 1)\n",
    "        outputs = F.tanh(self.linear_first(outputs))\n",
    "        outputs = self.linear_second(outputs)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN2RNN(nn.Module):\n",
    "    def __init__(self, embedding_dim, max_len, num_layers, rate):\n",
    "        super(CNN2RNN, self).__init__()\n",
    "        self.cnn = CNN_Encoder(embedding_dim, rate)\n",
    "        self.rnn = RNN_Decoder(max_len, embedding_dim, num_layers, rate)\n",
    "        \n",
    "    def forward(self, img, seq):\n",
    "        cnn_output = self.cnn(img)\n",
    "        output = self.rnn(cnn_output, seq)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN2RNN(embedding_dim=embedding_dim, max_len=max_len, num_layers=num_layers, rate=dropout_rate)\n",
    "model = nn.DataParallel(model)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from adamp import AdamP\n",
    "\n",
    "# # define your params\n",
    "# optimizer = AdamP(model.parameters(), lr=1e-3, betas=(0.9, 0.999), weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5C_GVYbcp_wl"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=1e-6)\n",
    "criterion1 = nn.SmoothL1Loss()\n",
    "criterion2 = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2-jW8B5qp_wm"
   },
   "outputs": [],
   "source": [
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "def train_step(batch_item, epoch, batch, training):\n",
    "    img = batch_item['img'].to(device)\n",
    "    seq = batch_item['seq'].to(device)\n",
    "    label = batch_item['label'].to(device)\n",
    "    if training is True:\n",
    "        model.train()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(img, seq)\n",
    "        \n",
    "        with torch.cuda.amp.autocast():\n",
    "#             loss1 = criterion1(output, label)\n",
    "            loss = criterion2(output, label)\n",
    "#             loss = loss1 + loss2\n",
    "#             loss = 0.7*loss1 + 0.3*loss2\n",
    "\n",
    "        scaler.scale(loss).backward()  \n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5) \n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        scheduler.step()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    else:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            output = model(img, seq)\n",
    "#             loss1 = criterion1(output, label)\n",
    "            loss = criterion2(output, label)\n",
    "#             loss = loss1 + loss2\n",
    "#             loss = 0.7*loss1 + 0.3*loss2\n",
    "            \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ConcatDataset([train_dataset, val_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_plot, val_loss_plot = [], []\n",
    "\n",
    "# from sklearn.model_selection import KFold\n",
    "# kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# best_models = []\n",
    "# for fold, (trn_idx, val_idx) in enumerate(kfold.split(dataset)):\n",
    "#     print(f'[fold: {fold}]')\n",
    "    \n",
    "#     torch.cuda.empty_cache()\n",
    "    \n",
    "#     train_sampler = SubsetRandomSampler(trn_idx)\n",
    "#     val_sampler = SubsetRandomSampler(val_idx)\n",
    "    \n",
    "    \n",
    "#     train_dataloader = torch.utils.data.DataLoader(dataset,\n",
    "#                                                batch_size=BATCH_SIZE,\n",
    "#                                                num_workers=12,\n",
    "#                                                 pin_memory=True,\n",
    "#                                                 sampler=train_sampler)\n",
    "#     val_dataloader = torch.utils.data.DataLoader(dataset,\n",
    "#                                              batch_size=BATCH_SIZE,\n",
    "#                                              num_workers=12,\n",
    "#                                                 pin_memory=True,\n",
    "#                                                 sampler=val_sampler)\n",
    "    \n",
    "#     for epoch in range(EPOCHS):\n",
    "#         total_loss, total_val_loss = 0, 0\n",
    "\n",
    "#         tqdm_dataset = tqdm(enumerate(train_dataloader))\n",
    "#         training = True\n",
    "#         for batch, batch_item in tqdm_dataset:\n",
    "#             batch_loss = train_step(batch_item, epoch, batch, training)\n",
    "#             total_loss += batch_loss\n",
    "\n",
    "#             tqdm_dataset.set_postfix({\n",
    "#                 'Fold': fold + 1,\n",
    "#                 'Epoch': epoch + 1,\n",
    "#                 'Loss': '{:06f}'.format(batch_loss.detach()),\n",
    "#                 'Total Loss' : '{:06f}'.format(total_loss/(batch+1))\n",
    "#             })\n",
    "#         loss_plot.append(total_loss/(batch+1))\n",
    "\n",
    "#         tqdm_dataset = tqdm(enumerate(val_dataloader))\n",
    "#         training = False\n",
    "#         for batch, batch_item in tqdm_dataset:\n",
    "#             batch_loss = train_step(batch_item, epoch, batch, training)\n",
    "#             total_val_loss += batch_loss\n",
    "\n",
    "#             tqdm_dataset.set_postfix({\n",
    "#                 'Fold': fold + 1,\n",
    "#                 'Epoch': epoch + 1,\n",
    "#                 'Val Loss': '{:06f}'.format(batch_loss.detach()),\n",
    "#                 'Total Val Loss' : '{:06f}'.format(total_val_loss/(batch+1))\n",
    "#             })\n",
    "#         val_loss_plot.append(total_val_loss/(batch+1))\n",
    "\n",
    "#         if np.min(val_loss_plot) == val_loss_plot[-1]:\n",
    "#     #         torch.save(model, save_path)\n",
    "#             torch.save({'state_dict': model.state_dict()}, '/home/ubuntu/Samsung_Dacon/models/Tf_EfficientB1/tf_efficientnet_b1-fold{}.pt'.format(fold))\n",
    "# #             torch.save({'state_dict': model.state_dict()}, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss_plot, val_loss_plot = [], []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss, total_val_loss = 0, 0\n",
    "\n",
    "    tqdm_dataset = tqdm(enumerate(train_dataloader))\n",
    "    training = True\n",
    "    for batch, batch_item in tqdm_dataset:\n",
    "        batch_loss = train_step(batch_item, epoch, batch, training)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        tqdm_dataset.set_postfix({\n",
    "            'Epoch': epoch + 1,\n",
    "            'Loss': '{:06f}'.format(batch_loss.detach()),\n",
    "            'Total Loss' : '{:06f}'.format(total_loss/(batch+1))\n",
    "        })\n",
    "    loss_plot.append(total_loss/(batch+1))\n",
    "\n",
    "    tqdm_dataset = tqdm(enumerate(val_dataloader))\n",
    "    training = False\n",
    "    for batch, batch_item in tqdm_dataset:\n",
    "        batch_loss = train_step(batch_item, epoch, batch, training)\n",
    "        total_val_loss += batch_loss\n",
    "\n",
    "        tqdm_dataset.set_postfix({\n",
    "            'Epoch': epoch + 1,\n",
    "            'Val Loss': '{:06f}'.format(batch_loss.detach()),\n",
    "            'Total Val Loss' : '{:06f}'.format(total_val_loss/(batch+1))\n",
    "        })\n",
    "    val_loss_plot.append(total_val_loss/(batch+1))\n",
    "\n",
    "    if np.min(val_loss_plot) == val_loss_plot[-1]:\n",
    "#         torch.save(model, save_path)\n",
    "#             torch.save({'state_dict': model.state_dict()}, '/home/ubuntu/Samsung_Dacon/models/Tf_EfficientB1/tf_efficientnet_b1-fold{}.pt'.format(fold))\n",
    "        torch.save({'state_dict': model.state_dict()}, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "mn7sFqbSO4ag"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_plot, label='train_loss')\n",
    "plt.plot(val_loss_plot, label='val_loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss(mae)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 복원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = CNN2RNN(embedding_dim=embedding_dim, max_len=max_len, num_layers=num_layers, rate=dropout_rate)\n",
    "# model = nn.DataParallel(model)\n",
    "# model = model.to(device)\n",
    "\n",
    "# import copy\n",
    "# best_models=[]\n",
    "# save_path = f'/home/ubuntu/Samsung_Dacon/models/Tf_EfficientB1/tf_efficientnet_b1-fold1.pt'\n",
    "# checkpoint = torch.load(save_path)\n",
    "# model.load_state_dict(checkpoint['state_dict'])\n",
    "# best_models.append(copy.deepcopy(model))\n",
    "\n",
    "# save_path2 = f'/home/ubuntu/Samsung_Dacon/models/Tf_EfficientB1/tf_efficientnet_b1-fold2.pt'\n",
    "# checkpoint = torch.load(save_path2)\n",
    "# model.load_state_dict(checkpoint['state_dict'])\n",
    "# best_models.append(copy.deepcopy(model))\n",
    "\n",
    "# save_path3 = f'/home/ubuntu/Samsung_Dacon/models/Tf_EfficientB1/tf_efficientnet_b1-fold3.pt'\n",
    "# checkpoint = torch.load(save_path3)\n",
    "# model.load_state_dict(checkpoint['state_dict'])\n",
    "# best_models.append(copy.deepcopy(model))\n",
    "\n",
    "# save_path4 = f'/home/ubuntu/Samsung_Dacon/models/Tf_EfficientB1/tf_efficientnet_b1-fold4.pt'\n",
    "# checkpoint = torch.load(save_path4)\n",
    "# model.load_state_dict(checkpoint['state_dict'])\n",
    "# best_models.append(copy.deepcopy(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save_path = f'/home/ubuntu/Samsung_Dacon/models/Tf_EfficientB1/tf_efficientnet_b1-fold4.pt'\n",
    "checkpoint = torch.load(save_path)\n",
    "model.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 테스트 데이터 및 제출 양식 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('/home/ubuntu/Samsung_Dacon/test.csv')\n",
    "submission = pd.read_csv('/home/ubuntu/Samsung_Dacon/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 테스트 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx, row in tqdm(test.iterrows()):\n",
    "#     file = row['uid']\n",
    "#     smiles = row['SMILES']\n",
    "#     m = Chem.MolFromSmiles(smiles)\n",
    "#     if m != None:\n",
    "#         img = Draw.MolToImage(m, size=(300,300))\n",
    "#         img.save(f'/home/ubuntu/Samsung_Dacon/test_imgs/{file}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 테스트 데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 602/602 [00:00<00:00, 42184.80it/s]\n"
     ]
    }
   ],
   "source": [
    "test_seqs = tokenizer.txt2seq(test.SMILES)\n",
    "test_imgs = ('/home/ubuntu/Samsung_Dacon/test_imgs/'+test.uid+'.png').to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomDataset(imgs=test_imgs,\n",
    "                             seqs=test_seqs,\n",
    "                             labels=None, mode='test',\n",
    "                            transform=get_validation_augmentation(),\n",
    "                           preprocessing=get_preprocessing(preprocessing_fn)\n",
    "                            )\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                              batch_size=BATCH_SIZE,\n",
    "                                              num_workers=12,\n",
    "                                             pin_memory=True,\n",
    "                                             shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset1 = CustomDataset(imgs=test_imgs,\n",
    "#                              seqs=test_seqs,\n",
    "#                              labels=None, mode='test',\n",
    "#                             transform=None,\n",
    "#                           preprocessing=get_preprocessing(preprocessing_fn)\n",
    "#                             )\n",
    "\n",
    "# test_dataloader1 = torch.utils.data.DataLoader(test_dataset1,\n",
    "#                                               batch_size=BATCH_SIZE,\n",
    "#                                               num_workers=12,\n",
    "#                                              pin_memory=True,\n",
    "#                                              shuffle=False)\n",
    "\n",
    "# test_dataset2 = CustomDataset(imgs=test_imgs,\n",
    "#                              seqs=test_seqs,\n",
    "#                              labels=None, mode='test',\n",
    "#                             transform=get_validation_augmentation(),\n",
    "#                            preprocessing=get_preprocessing(preprocessing_fn)\n",
    "#                             )\n",
    "\n",
    "# test_dataloader2 = torch.utils.data.DataLoader(test_dataset2,\n",
    "#                                               batch_size=BATCH_SIZE,\n",
    "#                                               num_workers=12,\n",
    "#                                              pin_memory=True,\n",
    "#                                              shuffle=False)\n",
    "\n",
    "# test_dataset3 = CustomDataset(imgs=test_imgs,\n",
    "#                              seqs=test_seqs,\n",
    "#                              labels=None, mode='test',\n",
    "#                             transform=get_validation_augmentation(),\n",
    "#                            preprocessing=get_preprocessing(preprocessing_fn)\n",
    "#                             )\n",
    "\n",
    "# test_dataloader3 = torch.utils.data.DataLoader(test_dataset3,\n",
    "#                                               batch_size=BATCH_SIZE,\n",
    "#                                               num_workers=12,\n",
    "#                                              pin_memory=True,\n",
    "#                                              shuffle=False)\n",
    "\n",
    "# test_dataset4 = CustomDataset(imgs=test_imgs,\n",
    "#                              seqs=test_seqs,\n",
    "#                              labels=None, mode='test',\n",
    "#                             transform=get_validation_augmentation(),\n",
    "#                            preprocessing=get_preprocessing(preprocessing_fn)\n",
    "#                             )\n",
    "\n",
    "# test_dataloader4 = torch.utils.data.DataLoader(test_dataset4,\n",
    "#                                               batch_size=BATCH_SIZE,\n",
    "#                                               num_workers=12,\n",
    "#                                              pin_memory=True,\n",
    "#                                              shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 추론 및 제출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(dataset):\n",
    "    model.eval()\n",
    "    result = []\n",
    "    for batch_item in dataset:\n",
    "        img = batch_item['img'].to(device)\n",
    "        seq = batch_item['seq'].to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(img, seq)\n",
    "        output = output.cpu().numpy()\n",
    "#         gap = (output[:, 0] - output[:, 1]) # + output[:,2])/2.0\n",
    "        gap = output[:,0]\n",
    "        gap = np.where(gap<0, 0, gap)\n",
    "        result.extend(list(gap))\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction_list=[]\n",
    "# prediction_df = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "# for test_dataloader in [test_dataloader1, test_dataloader2, test_dataloader3, test_dataloader4]:\n",
    "#     for model in best_models:\n",
    "#         prediction_array = np.zeros([prediction_df.shape[0],\n",
    "#                                 prediction_df.shape[1] -1])      \n",
    "#         for idx, sample in enumerate(test_dataloader):\n",
    "#             model.eval()\n",
    "#             img = sample['img'].to(device)\n",
    "#             seq = sample['seq'].to(device)\n",
    "#             with torch.no_grad():\n",
    "#                 probs = model(img, seq)\n",
    "#             probs = probs.cpu().numpy()\n",
    "#             gap = probs[:, 0]\n",
    "#         prediction_list.append(gap)\n",
    "# #             prediction_array[idx*BATCH_SIZE: (idx+1)*BATCH_SIZE,:]\\\n",
    "# #                       = gap.astype(np.float32)\n",
    "                \n",
    "# #         predictions_list.append(prediction_array[...,np.newaxis])\n",
    "\n",
    "# # # axis = 2를 기준으로 평균 (TTA, folds for each SEED)\n",
    "# # predictions_array = np.concatenate(predictions_list, axis = 2)\n",
    "# # predictions_mean = predictions_array.mean(axis = 2)\n",
    "\n",
    "# # # 평균 값이 0.5보다 클 경우 1 작으면 0\n",
    "# # predictions_mean = (predictions_mean > 0.5) * 1\n",
    "# # predictions_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predict(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['ST1_GAP(eV)'] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>ST1_GAP(eV)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_0</td>\n",
       "      <td>0.782390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_1</td>\n",
       "      <td>2.250865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_2</td>\n",
       "      <td>0.899378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_3</td>\n",
       "      <td>0.595134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_4</td>\n",
       "      <td>1.230900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>test_597</td>\n",
       "      <td>0.124859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>test_598</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>test_599</td>\n",
       "      <td>0.031120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>test_600</td>\n",
       "      <td>0.010209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601</th>\n",
       "      <td>test_601</td>\n",
       "      <td>0.602933</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>602 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          uid  ST1_GAP(eV)\n",
       "0      test_0     0.782390\n",
       "1      test_1     2.250865\n",
       "2      test_2     0.899378\n",
       "3      test_3     0.595134\n",
       "4      test_4     1.230900\n",
       "..        ...          ...\n",
       "597  test_597     0.124859\n",
       "598  test_598     0.000000\n",
       "599  test_599     0.031120\n",
       "600  test_600     0.010209\n",
       "601  test_601     0.602933\n",
       "\n",
       "[602 rows x 2 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('dacon_baseline.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "제출 API 사용법 => https://dacon.io/forum/403557"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dacon_submit_api import dacon_submit_api \n",
    "\n",
    "# result = dacon_submit_api.post_submission_file(\n",
    "#     '/home/ubuntu/Samsung_Dacon/dacon_baseline.csv', \n",
    "#     '19342bef1dc6b917ad92a59c02df55db3395fcbe58c0ccdc5eb5761d9a27d5f7', \n",
    "#     '235789', \n",
    "#     '', \n",
    "#     'DACON_Baseline'\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Public LB Score : 0.1871130054"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
